% Encoding: UTF-8

@article{Haykin2001 ,
abstract = {As redes neurais artificiais t{\^{e}}m ra{\'{i}}zes em disciplinas como neuroci{\^{e}}ncia, matem{\'{a}}tica, estat{\'{i}}stica, f{\'{i}}sica, ci{\^{e}}ncia da computa{\c{c}}{\~{a}}o e engenharia. Suas aplica{\c{c}}{\~{o}}es podem ser encontradas em campos t{\~{a}}o diversos quanto modelagem, an{\'{a}}lise de s{\'{e}}ries temporais, reconhecimento de padr{\~{o}}es, processamento de sinais e controle. Este livro fornece as bases para o entendimento das redes neurais, reconhecendo a natureza multidisciplinar do tema.},
author = {Haykin, Simon},
pages = {901},
title = {{Redes Neurais - Princ{\'{i}}pios e pr{\'{a}}tica}},
volume = {2},
year = {2001}
}

@article{Cintra2019,
author = {Cintra, Rosangela},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cintra - Unknown - Introdu{\c{c}}{\~{a}}o {\`{a}} Neurcomputa{\c{c}}{\~{a}}o.pdf:pdf},
journal = {ELAC 2019},
keywords = {especialistas},
pages = {22},
title = {{Introdu{\c{c}}{\~{a}}o {\`{a}} Neurocomputa{\c{c}}{\~{a}}o}},
year = {2019}
}

@article{Nelson2017,
abstract = {Predictions on stock market prices are a great challenge due to the fact that it is an immensely complex, chaotic and dynamic environment. There are many studies from various areas aiming to take on that challenge and Machine Learning approaches have been the focus of many of them. There are many examples of Machine Learning algorithms being able to reach satisfactory results when doing that type of prediction. This project studies the usage of LSTM networks on that scenario, to predict future trends of stock prices based on the price history, alongside with technical analysis indicators. For that goal, a prediction model was built, and a series of experiments were executed and theirs results analyzed against a number of metrics to assess if this type of algorithm presents any improvements when compared to other Machine Learning methods and investment strategies. The results that were obtained are promising, getting up to an average of 55.9{\%} of accuracy when predicting if the price of a particular stock is going to go up or not in the near future. This model was also assessed under financial aspects, showing promissing results in terms of financial returns.},
author = {Nelson, David Michael Quirino},
keywords = {Artificial Neural Networks,Deep Learning,Long Short-Term Memory,Machine Learning,Recurrent Neural Networks,Stock Markets,Technical Analysis},
pages = {55},
title = {{Uso de redes neurais recorrentes para previs{\~{a}}o de s{\'{e}}ries temporais financeiras}},
year = {2017}
}

@article{Osorio1999,
abstract = {Forum de I.A. '99 – Pg.1 Introdu{\c{c}}{\~{a}}o Este tutorial tem por objetivo apresentar uma introdu{\c{c}}{\~{a}}o ao aprendizado artificial e automatizado (machine learning), focalizando-se sobre os aspectos referentes a uma t{\'{e}}cnica em particular, as redes neurais artificiais –R.N.A. Na primeira se{\c{c}}{\~{a}}o vamos discutir sobre a Intelig{\^{e}}ncia Artificial, sobre a aquisi{\c{c}}{\~{a}}o de conhecimentos e sobre a import{\^{a}}ncia do aprendizado na constru{\c{c}}{\~{a}}o de sistemas inteligentes. Na segunda se{\c{c}}{\~{a}}o iremos abordar as redes neurais artificiais (modelos conexionistas), onde vamos destacar: os diferentes tipos de redes e de algoritmos de aprendizado existentes; a representa{\c{c}}{\~{a}}o do conhecimento neural; as caracter{\'{i}}sticas e limita{\c{c}}{\~{o}}es de uso deste tipo de t{\'{e}}cnicas, bem como mostraremos alguns exemplos de aplica{\c{c}}{\~{o}}es das RNAs. Para concluir, iremos discutir sobre os caminhos da pesquisa atual nesta {\'{a}}rea e tend{\^{e}}ncias futuras no que diz respeito ao desenvolvimento dos sistemas inteligentes.},
author = {Os{\'{o}}rio, Fernando},
journal = {I Forum De Intelig{\^{e}}ncia Artificial},
number = {1},
pages = {1--32},
title = {{Redes Neurais - Aprendizado Artificial}},
url = {http://osorio.wait4.org/oldsite/IForumIA/fia99.pdf},
year = {1999}
}

@BOOK {livroNunes2016,
    author    = "Silva, Ivan Nunes da and Spatti, Danilo Hernane and Flauzino, Rogério Andrade",
    title     = "Redes Neurais Artificiais Para Engenharia e Ciências Aplicadas. Fundamentos Teóricos e Aspectos Práticos",
    publisher = "Artliber",
    year      = "2016",
    edition   = "second",
    month     = "dec"
}

@inbook{bezerra2016,
author = {Bezerra, Eduardo},
file = {:home/felipe/Documentos/livros/sbbd2016-intro-deep-learning.pdf:pdf},
pages = {57--86},
title = {{Introdu{\c{c}}{\~{a}}o {\`{a}} Aprendizagem Profunda}},
year = {2016}
}

@article{Camila2017,
author = {da Silva, Larrisa Camila Ferreira},
file = {:home/felipe/Documentos/livros/lcfs{\_}tg.pdf:pdf},
title = {{Modelo de Transfer{\^{e}}ncia de Aprendizagem baseado em Regress{\~{a}}o Linear Regularizada}},
year = {2017}
}

@book{Ponti2018,
abstract = {Deep Learning methods are currently the state-of-the-art in many problems which can be tackled via machine learning, in particular classification problems. However there is still lack of understanding on how those methods work, why they work and what are the limitations involved in using them. In this chapter we will describe in detail the transition from shallow to deep networks, include examples of code on how to implement them, as well as the main issues one faces when training a deep network. Afterwards, we introduce some theoretical background behind the use of deep models, and discuss their limitations.},
archivePrefix = {arXiv},
arxivId = {1806.07908},
author = {Ponti, Moacir Antonelli and da Costa, Gabriel B. Paranhos},
eprint = {1806.07908},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponti, da Costa - 2018 - Como funciona o Deep Learning.pdf:pdf},
isbn = {9788576694007},
pages = {63--93},
title = {{Como funciona o Deep Learning}},
url = {http://arxiv.org/abs/1806.07908},
year = {2018}
}

@article{Carneiro2017,
abstract = {Convolutional Neural Networks (CNNs) belong to a category of algorithms based on artificial neural networks that use convolution in at least one of their layers. CNNs have proven to be efficient in various tasks of image and video recognition, recommendation systems and natural language processing, however they need a large number of labeled samples for learning. Thus, in addition to introducing the main concepts and components of CNNs, this chapter also discusses how to use transfer learning techniques to accelerate CNN training process or extract features of databases that do not have sufficient images for the training. Finally, we present a tutorial in python on how to use the Tensorflow library to build and run a CNN to classify images. Resumo},
author = {Araújo, Flávio H. D and Carneiro, Allan C and Silva, Romuere R. V and Medeiros, Fátima N. S. and Ushizima, Daniela M.},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carneiro, Silva - 2017 - Redes Neurais Convolucionais com Tensorflow Teoria e Pr{\'{a}}tica.pdf:pdf},
isbn = {9788576693956},
journal = {III Escola Regional de Inform{\'{a}}tica do Piau{\'{i}}},
pages = {382--406},
title = {{Redes Neurais Convolucionais com Tensorflow: Teoria e Pr{\'{a}}tica}},
year = {2017}
}

@article{decio2017,
author = {Neto, Décio Gonçalves de Aguiar},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/D{\'{E}}CIO GON{\c{C}}ALVES DE AGUIAR NETO - 2017 - TRANSFER{\^{E}}NCIA DE CONHECIMENTO UTILIZANDO APRENDIZADO PROFUNDO PARA CLASSIFICA{\c{C}}{\~{A}}O DE IMAGEN.pdf:pdf},
pages = {399--404},
title = {{TRANSFER{\^{E}}NCIA DE CONHECIMENTO UTILIZANDO APRENDIZADO PROFUNDO PARA CLASSIFICA{\c{C}}{\~{A}}O DE IMAGENS HISTOPATOL{\'{O}}GICAS}},
year = {2017}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Shankar2017,
abstract = {In this paper, we present a unified end-to-end approach to build a large scale Visual Search and Recommendation system for e-commerce. Previous works have targeted these problems in isolation. We believe a more effective and elegant solution could be obtained by tackling them together. We propose a unified Deep Convolutional Neural Network architecture, called VisNet, to learn embeddings to capture the notion of visual similarity, across several semantic granularities. We demonstrate the superiority of our approach for the task of image retrieval, by comparing against the state-of-the-art on the Exact Street2Shop dataset. We then share the design decisions and trade-offs made while deploying the model to power Visual Recommendations across a catalog of 50M products, supporting 2K queries a second at Flipkart, India's largest e-commerce company. The deployment of our solution has yielded a significant business impact, as measured by the conversion-rate.},
archivePrefix = {arXiv},
arxivId = {1703.02344},
author = {Shankar, Devashish and Narumanchi, Sujay and Ananya, H A and Kompalli, Pramod and Chaudhury, Krishnendu},
doi = {10.1029/2001JD000377},
eprint = {1703.02344},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shankar et al. - 2017 - Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce.pdf:pdf},
issn = {0148-0227},
keywords = {c,computer vision,deep learning,detail based similarity via,distributed systems,e-commerce,image retrieval,recommender systems,visual search},
pmid = {18663850},
title = {{Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce}},
url = {http://arxiv.org/abs/1703.02344},
year = {2017}
}

@inproceedings{Krizhevsky2012,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {1097--1105},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
 acmid = {2999257},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@article{Caroline2016,
abstract = {Resumo—Este trabalho aborda o tema de Aprendizado Pro-fundo com um olhar d{\'{a}} area de Vis{\~{a}}o Computacional, tra{\c{c}}ando um paralelo entre as dua areas para a tarefa de detec ao de pedestres. Para tanto, os resultados de duas abordagens cl{\'{a}}ssicas de detec ao de pedestres s{\~{a}}o comparados experimentalmente com uma abordagem de Redes Neurais Convolucionais, cujos resultados s{\~{a}}o considerados o estado da arte para o problema em quest{\~{a}}o. Keywords-Detec ao de Pedestres, Vis{\~{a}}o Computacional, Apren-dizado Profundo, CNN, SVM, HOG, Adaboost, Haar features Abstract—This work addresses Deep Learning from the point of view of the Computer Vision area, drawing a parallel between the two areas, considering the pedestrian detection task. To achieve that, the experimental results of two classical Computer Vision approaches are compared to the results obtained from a Convolutional Neural Network, which is known for obtaining the state of the art to the problem of pedestrian detection.},
author = {Vargas, Ana Caroline and Paes, Aline and Vasconcelos, Cristina Nader},
file = {:home/felipe/Documentos/livros/um-estudo-sobre.pdf:pdf},
isbn = {142440357X},
keywords = {Adaboost,CNN,Computer Vision,HOG,Haar features.,Machine Learning,Pedestrian Detection,SVM},
pages = {4},
title = {{Um Estudo sobre Redes Neurais Convolucionais e sua Aplica{\c{c}}{\~{a}}o em Detec{\c{c}}{\~{a}}o de Pedestres}},
url = {http://sibgrapi.sid.inpe.br/col/sid.inpe.br/sibgrapi/2016/09.12.15.44/doc/um-estudo-sobre.pdf},
year = {2016}
}

@article{Miyazaki2017,
author = {Miyazaki, Caio Kioshi},
file = {:home/felipe/Documentos/livros/Miyazaki{\_}caio{\_}tcc.pdf:pdf},
title = {{Redes neurais convolucionais para aprendizagem e reconhecimento de objetos 3D}},
year = {2017}
}

@article{LeCun1998,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengion, Yoshua and Haffner, Patrick},
file = {:home/felipe/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
title = {{Gradient-Based Learning Applied to Document Recognition}},
url = {http://ieeexplore.ieee.org/document/726791/{\#}full-text-section},
year = {1998}
}

@article{VonZuben2013,
author = {{Von Zuben}, Fernando J.},
file = {:home/felipe/Documentos/livros/introducao{\_}EA072{\_}2s2013.pdf:pdf},
keywords = {que pensam como},
pages = {1--48},
title = {{Introdu{\c{c}}{\~{a}}o {\`{a}} Intelig{\^{e}}ncia Artificial}},
year = {2013}
}

@BOOK{Winston1992,
    author    = "Winston, Patrick Henry",
    title     = "Artificial Intelligence",
    publisher = "Addison-Wesley",
    year      = "1992",
    volume    = "3"
}

@MISC {Augusto2007,
    author = "José Augusto Baranauskas",
    title  = "Aprendizado de Máquina Conceitos e Definições",
    year   = "2007"
}

@TECHREPORT {Amidi2018,
    author      = "Amidi, Shervine",
    title       = "Convolutional Neural Networks cheatsheet",
    institution = "Stanford",
    year        = "2018"
}

@article{rawat2017,
author = {Rawat, Waseem and Wang, Zenghui},
doi = {10.1162/NECO_a_00990},
journal = {Neural Computation},
pages = {1--98},
title = {{Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review}},
volume = {29},
year = {2017}
}

@book{Bishop2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher},
booktitle = {Journal of Organic Chemistry},
doi = {10.1021/jo01026a014},
eprint = {arXiv:1011.1669v3},
file = {:home/felipe/Documentos/livros/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:pdf},
isbn = {9780387310732},
issn = {0022-3263},
pmid = {25246403},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{george2017,
  author    = {George Papandreou and
               Tyler Zhu and
               Nori Kanazawa and
               Alexander Toshev and
               Jonathan Tompson and
               Chris Bregler and
               Kevin P. Murphy},
  title     = {Towards Accurate Multi-person Pose Estimation in the Wild},
  journal   = {CoRR},
  volume    = {abs/1701.01779},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.01779},
  archivePrefix = {arXiv},
  eprint    = {1701.01779},
  timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PapandreouZKTTB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{george2018,
  author    = {George Papandreou and
               Tyler Zhu and
               Liang{-}Chieh Chen and
               Spyros Gidaris and
               Jonathan Tompson and
               Kevin Murphy},
  title     = {PersonLab: Person Pose Estimation and Instance Segmentation with a
               Bottom-Up, Part-Based, Geometric Embedding Model},
  journal   = {CoRR},
  volume    = {abs/1803.08225},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.08225},
  archivePrefix = {arXiv},
  eprint    = {1803.08225},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-08225},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

@misc{chollet2015,
author = {François Chollet },
title = {keras},
year = {2015},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/fchollet/keras}},
commit = {5bcac37}
}

@ONLINE {colab,
    author = "Google",
    title  = "Google Colaboratory",
    url    = "https://research.google.com/colaboratory/faq.html"
}

@Article{PER-GRA:2007,
  Author    = {P\'erez, Fernando and Granger, Brian E.},
  Title     = {{IP}ython: a System for Interactive Scientific Computing},
  Journal   = {Computing in Science and Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {21--29},
  month     = may,
  year      = 2007,
  url       = "http://ipython.org",
  ISSN      = "1521-9615",
  doi       = {10.1109/MCSE.2007.53},
  publisher = {IEEE Computer Society},
}

@article{Carneiro2018,
abstract = {Google Colaboratory (also known as Colab) is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research. It provides a runtime fully configured for deep learning and free-of-charge access to a robust GPU. This paper presents a detailed analysis of Colaboratory regarding hardware resources, performance, and limitations. This analysis is performed through the use of Colaboratory for accelerating deep learning for computer vision and other GPU-centric applications. The chosen test-cases are a parallel tree-based combinatorial search and two computer vision applications: object detection/classification and object localization/segmentation. The hardware under the accelerated runtime is compared with a mainstream workstation and a robust Linux server equipped with 20 physical cores. Results show that the performance reached using this cloud service is equivalent to the performance of the dedicated testbeds, given similar resources. Thus, this service can be effectively exploited to accelerate not only deep learning but also other classes of GPU-centric applications. For instance, it is faster to train a CNN on Colaboratory's accelerated runtime than using 20 physical cores of a Linux server. The performance of the GPU made available by Colaboratory may be enough for several profiles of researchers and students. However, these free-of-charge hardware resources are far from enough to solve demanding real-world problems and are not scalable. The most significant limitation found is the lack of CPU cores. Finally, several strengths and limitations of this cloud service are discussed, which might be useful for helping potential users.},
author = {Carneiro, Tiago and {Da Nobrega}, Raul Victor Medeiros and Nepomuceno, Thiago and Bian, Gui Bin and {De Albuquerque}, Victor Hugo C. and Filho, Pedro Pedrosa Reboucas},
doi = {10.1109/ACCESS.2018.2874767},
file = {:home/felipe/Documentos/livros/Performance{\_}Analysis{\_}of{\_}Google{\_}Colaboratory{\_}as{\_}a{\_}T.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Colab,Deep learning,GPU computing,Google colaboratory,convolutional neural networks},
pages = {61677--61685},
title = {{Performance Analysis of Google Colaboratory as a Tool for Accelerating Deep Learning Applications}},
volume = {6},
year = {2018}
}

@article{tensorflowjs2019,
  author    = {Daniel Smilkov and
               Nikhil Thorat and
               Yannick Assogba and
               Ann Yuan and
               Nick Kreeger and
               Ping Yu and
               Kangyi Zhang and
               Shanqing Cai and
               Eric Nielsen and
               David Soergel and
               Stan Bileschi and
               Michael Terry and
               Charles Nicholson and
               Sandeep N. Gupta and
               Sarah Sirajuddin and
               D. Sculley and
               Rajat Monga and
               Greg Corrado and
               Fernanda B. Vi{\'{e}}gas and
               Martin Wattenberg},
  title     = {TensorFlow.js: Machine Learning for the Web and Beyond},
  journal   = {CoRR},
  volume    = {abs/1901.05350},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.05350},
  archivePrefix = {arXiv},
  eprint    = {1901.05350},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-05350},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unknown{Remes2016,
author = {Remes, Chrystian},
doi = {10.13140/RG.2.1.3530.5849},
title = {{Caracteriza{\c{c}}{\~{a}}o Por Simula{\c{c}}{\~{a}}o Num{\'{e}}rica de Pain{\'{e}}is Fotovoltaicos e M{\'{e}}todo de Rastreamento do M{\'{a}}ximo Ponto de Pot{\^{e}}ncia Baseado em Redes Neurais Artificiais.}},
year = {2016}
}


% @Comment{jabref-meta: databaseType:bibtex;}
